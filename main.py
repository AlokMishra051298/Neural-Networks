# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rxBM7mhxEuwV0EvzqNDYRax3iX2Tl5B9
"""
import random
import numpy
#from numpy import *


class Network(object):
  
    
  def __init__(self,sizes):
    self.num_layers=len(sizes)
    self.sizes=sizes
    self.biases=[numpy.random.randn(y,1) for y in sizes[1:]]
    self.weights=[numpy.random.randn(y,x)
                 for x,y in zip(sizes[:-1],sizes[1:])]
  
  '''-----------------------------------------------------'''
  
  #"a" is input
  def feedforward(self,a):
    '''Return the output of the network if "a" is input '''
  
    for w,b in zip(self.weights, self.biases):
      a=sigmoid(numpy.dot(w,a)+b)#here we find out the dot product of vectors "w","a" and then add the value to "b"
    return a 
  
  '''-----------------------------------------------------------------------
  '''
  
  def SGD(self,training_data,epoachs, mini_batch_size,eta,test_data=None):
  
  
    if test_data: 
      n_test=len(test_data)
    n=len(training_data)#the training_data camme from after loading MNIST dataset
  
  
    #Epoachs is basically how many times you give the data to your model
    for j in range(epoachs):
      random.shuffle(list(training_data))#shuffle the training data
  
  
      #mini_batches contains the small sized batches of training data which decreases the computation time 
      mini_batches=[training_data[k:k+mini_batch_size]
                   for k in range(0,n,mini_batch_size)]#range(start,end,mini_batch_size)....k=0,100,200,300,400,500,600,700,...if mini_batch_size=100
      for mini_batch in mini_batches:#take single mini_batch from array "mini_batches[]"
  
        '''for each mini_batch we apply a single step of gradient descent,
        which is done by the code "self.update_mini_batch(mini_batch, eta)", 
        which updates the network weights and biases according to a single iteration of gradient descent,
        using just the training data in mini_batch'''
  
  
        #the function update_mini_batch() is defined next after that function
        self.update_mini_batch(mini_batch,eta)#here eta is the η(learning rate)"
  
   
      if test_data:
        '''If the optional argument test_data is supplied, then the program will evaluate the network after each epoch of training, 
        and print out partial progress. 
        This is useful for tracking progress, but slows things down substantially.'''
      
      
        
        print("Epoach {0}:{1}{2}".format(j,
                                         self.evaluate(test_data),
                                        n_test) )
        '''j is currently executing epoach defined in upper loop which value passed into {0}'''
        '''self.evaluate(test_data) simply return the no. of test inputs for which neural network outputs the correct result'''
        '''n_test is the len(test_data)'''
        
      else:
        print("Epoach{0} complete".format(j))
      
  '''--------------------------------------------------------------------'''


  def update_mini_batch(self, mini_batch, eta):
    

    """Update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch.
    The mini_batch is a list of tuples "(x,y)", and eta is the learning rate."""

    #nabla refers to ∇ 
    '''nabla_w and nabla_b represent the total update for all of the weights and biases for this minibatch.'''

    nabla_b=[numpy.zeros(b.shape) for b in self.biases]#np.zeroes(shape,order,type) returns ndarray of zeros having given shape, order and datatype.
    nabla_w=[numpy.zeros(w.shape) for w in self.weights]
    z=[]

    for i in mini_batch:
      z.clear()
      for a,b in enumerate(i):
        z.append(b)
      x=z[0]
      y=z[1]

      delta_nabla_w, delta_nabla_b=self.backpropagation(x,y)
    #above backpropagation() function computes the gradient for all of the weights and biases for an example

      nabla_w=[dnw + nw for nw, dnw in zip(delta_nabla_w, nabla_w)]
      nabla_w=[dnb + nb for nb, dnb in zip(delta_nabla_b, nabla_b)]
    self.weights=[w-(eta/len(mini_batch))*nw
            for w, nw in zip(self.weights, nabla_w)]

    self.biases=[b-(eta/len(mini_batch))*nb
            for b,nb in zip(self.biases, nabla_b)]

    '''-----------------------------------------------'''
    
    #provoked by the SGD by passing the value of x,y as value of tuple (x,y) contained by mini_batch
  def backpropagation(self,x,y):
    '''Return the tuple 'nabla_w, nabla_b' representing the gradient for the cost function ∇C.
    nabla_w and nabla_b are layer by layer lists of numpy arrays, similiar 
    to "self.biases" and "self.weights".
    '''
    nabla_b=[numpy.zeros(b.shape) for b in self.biases]
    nabla_w=[numpy.zeros(w.shape) for w in self.weights]

    #feedforward
    activation=x.reshape(-1,1)
    y=y.reshape(-1,1)
    activations=[x.reshape(-1,1)]#list to store all the activations, layer by layer 
    zs=[]#list to store all the z-vector, layer by layer 


    #to calculate "z=w.activation+b" and store in zs 
    for b, w in zip(self.biases, self.weights):
      z=numpy.dot(w,activation)+b
      zs.append(z)
      activation=sigmoid(z)
      activations.append(activation)

    #backward pass 

    '''delta= (∂C_x/∂a_l)*(∂a_l / ∂z_l)'''
    delta_c=self.cost_derivative(activations[-1],y)
    delta=numpy.multiply(delta_c,sigmoid_prime(zs[-1])) 


    '''∂C_x/∂b_l=(∂z_l/∂b_l)*delta
    z_l=w_l * a_(l-1) + b_l
    so, ∂z_l/∂b_l = 1'''
    nabla_b[-1]=delta




    '''∂C_x/∂w_l=(∂z_l/∂w_l)*delta

       z_l=w_l * a_(l-1) + b_l

   so, ∂z_l/∂w_l = a_(l-1)'''
    nabla_w[-1]=numpy.dot(delta, activations[-2].transpose())

    # Note that the variable l in the loop below is used a little
    # Here,
    # l = 1 means the last layer of neurons, l = 2 is the
    # second-last layer, and so on.  It's a renumbering of the
    # scheme in the book, used here to take advantage of the fact
    # that Python can use negative indices in lists.
    for l in range(2,num_layers):
      z=zs[-1]
      sp=sigmoid_prime(zs[-1])

      '''here, delta is ∂C_x/∂a_(l-1)=(∂z_l/∂w_l)*(∂C_x/∂a_l)*(∂a_l / ∂z_l)
                                     = w_l * delta
                               '''
      delta=numpy.dot(self.weight[-l+1].transpose,delta)*sp  #confusion

      nabla_b[-l]=delta
      nabla_w[-l]=numpy.dot(delta,activations[-l-1].transpose())

    return ( nabla_b, nabla_w )

  
  '''-----------------------------------------------------------------------------
  '''
  
  def evaluate(self,test_data):
    '''Return the no. of test inputs for which the neural network outputs the correct result. 
    Note that the neural network's output is assumed to be the index of whichever neuron in the final layer has the activation. '''
  
    test_results=[(numpy.argmax(self.feedforward(x)),y)
               for (x,y) in test_data]
    return sum(int(x==y) for (x,y) in text_results)
  
  
  '''------------------------------------------------------------------------------'''
  
  
  def cost_derivative(self,output_activations,y):
    '''Returns the vector of partial derivatives  ∂C_x/∂a_l 
    here, C_x is cost of example x
    a_l is the output activations 
    '''
    cost_der=output_activations-y
    return (cost_der)

  '''--------------------------------------------------------------------------------------------------'''
  '''--------------------------------------------------------------------------------------------------------'''
  '''------------------------------------------------------------------------------------------------------------------'''
def sigmoid_prime(z):
  #returns the derivative of the sigmoid function 
  
  return sigmoid(z)*(1-sigmoid(z))
  '''------------------------------------------------------------------------'''

def sigmoid(z):
  return(1/(1+numpy.exp(-z)))

